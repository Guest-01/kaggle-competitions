{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('.venv': venv)"
  },
  "interpreter": {
   "hash": "2a911f678624b4b683fb9eba38552a022b6f34dc0cccfb4e8912a9803fd3006a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Tabular Playground Series - Aug 2021\n",
    "### Practice your ML skills on this approachable dataset!\n",
    "https://www.kaggle.com/c/tabular-playground-series-aug-2021/overview\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError"
   ]
  },
  {
   "source": [
    "## EDA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              f0   f1        f2        f3           f4        f5         f6  \\\n",
       "0      -0.002350   59  0.766739 -1.350460     42.27270  16.68570   30.35990   \n",
       "1       0.784462  145 -0.463845 -0.530421  27324.90000   3.47545  160.49800   \n",
       "2       0.317816   19 -0.432571 -0.382644   1383.26000  19.71290   31.10260   \n",
       "3       0.210753   17 -0.616454  0.946362   -119.25300   4.08235  185.25700   \n",
       "4       0.439671   20  0.968126 -0.092546     74.30200  12.30650   72.18600   \n",
       "...          ...  ...       ...       ...          ...       ...        ...   \n",
       "249995  0.923980    6  0.663212 -0.055120      6.61768   1.26619   40.44790   \n",
       "249996  0.243556    7 -0.557062  1.333470    -54.88610  17.58310  212.96400   \n",
       "249997  0.046023   53  0.462863  0.704034   2062.94000  14.58160   11.42710   \n",
       "249998  0.977330   12 -1.002880  0.576377   4741.16000  11.10090    3.81546   \n",
       "249999  0.244233   51  0.729304 -0.702592    197.46200  19.02400   42.33370   \n",
       "\n",
       "              f7          f8       f9  ...         f91        f92      f93  \\\n",
       "0       1.267300    0.392007  1.09101  ...   -42.43990  26.854000  1.45751   \n",
       "1       0.828007    3.735860  1.28138  ...  -184.13200   7.901370  1.70644   \n",
       "2      -0.515354   34.430800  1.24210  ...     7.43721  37.218100  3.25339   \n",
       "3       1.383310  -47.521400  1.09130  ...     9.66778   0.626942  1.49425   \n",
       "4      -0.233964   24.399100  1.10151  ...   290.65700  15.604300  1.73557   \n",
       "...          ...         ...      ...  ...         ...        ...      ...   \n",
       "249995  0.852375   25.522400  1.12294  ...  3517.97000  18.534200  1.41936   \n",
       "249996 -1.204750 -128.385000  1.19334  ...  1945.81000  19.482200  1.60072   \n",
       "249997 -0.509812   80.818000  1.20383  ...   -28.46890  11.008200  1.61080   \n",
       "249998  0.616191  118.902000  1.50424  ...  -275.53700  21.985700  3.16051   \n",
       "249999  0.465181 -115.398000  1.13675  ...  5387.25000  38.574600  3.61783   \n",
       "\n",
       "             f94        f95       f96       f97       f98        f99  loss  \n",
       "0       0.696161   0.941764  1.828470  0.924090  2.296580  10.489800    15  \n",
       "1      -0.494699  -2.058300  0.819184  0.439152  2.364700   1.143830     3  \n",
       "2       0.337934   0.615037  2.216760  0.745268  1.696790  12.305500     6  \n",
       "3       0.517513 -10.222100  2.627310  0.617270  1.456450  10.028800     2  \n",
       "4      -0.476668   1.390190  2.195740  0.826987  1.784850   7.071970     1  \n",
       "...          ...        ...       ...       ...       ...        ...   ...  \n",
       "249995  0.607504   4.950780  2.853420  0.961882  1.068010   1.437650    11  \n",
       "249996 -0.154877   5.693280  1.994540  0.760925  1.427880   1.226720     5  \n",
       "249997 -0.350927  27.986900  1.012690  0.830533  1.005470   0.455332     1  \n",
       "249998  0.772058   2.592220  3.938260  0.697430  0.962347   7.415890    10  \n",
       "249999 -0.544969  -0.987695  1.871910  0.452516  0.935411  28.912200     7  \n",
       "\n",
       "[250000 rows x 101 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>f0</th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>f5</th>\n      <th>f6</th>\n      <th>f7</th>\n      <th>f8</th>\n      <th>f9</th>\n      <th>...</th>\n      <th>f91</th>\n      <th>f92</th>\n      <th>f93</th>\n      <th>f94</th>\n      <th>f95</th>\n      <th>f96</th>\n      <th>f97</th>\n      <th>f98</th>\n      <th>f99</th>\n      <th>loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.002350</td>\n      <td>59</td>\n      <td>0.766739</td>\n      <td>-1.350460</td>\n      <td>42.27270</td>\n      <td>16.68570</td>\n      <td>30.35990</td>\n      <td>1.267300</td>\n      <td>0.392007</td>\n      <td>1.09101</td>\n      <td>...</td>\n      <td>-42.43990</td>\n      <td>26.854000</td>\n      <td>1.45751</td>\n      <td>0.696161</td>\n      <td>0.941764</td>\n      <td>1.828470</td>\n      <td>0.924090</td>\n      <td>2.296580</td>\n      <td>10.489800</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.784462</td>\n      <td>145</td>\n      <td>-0.463845</td>\n      <td>-0.530421</td>\n      <td>27324.90000</td>\n      <td>3.47545</td>\n      <td>160.49800</td>\n      <td>0.828007</td>\n      <td>3.735860</td>\n      <td>1.28138</td>\n      <td>...</td>\n      <td>-184.13200</td>\n      <td>7.901370</td>\n      <td>1.70644</td>\n      <td>-0.494699</td>\n      <td>-2.058300</td>\n      <td>0.819184</td>\n      <td>0.439152</td>\n      <td>2.364700</td>\n      <td>1.143830</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.317816</td>\n      <td>19</td>\n      <td>-0.432571</td>\n      <td>-0.382644</td>\n      <td>1383.26000</td>\n      <td>19.71290</td>\n      <td>31.10260</td>\n      <td>-0.515354</td>\n      <td>34.430800</td>\n      <td>1.24210</td>\n      <td>...</td>\n      <td>7.43721</td>\n      <td>37.218100</td>\n      <td>3.25339</td>\n      <td>0.337934</td>\n      <td>0.615037</td>\n      <td>2.216760</td>\n      <td>0.745268</td>\n      <td>1.696790</td>\n      <td>12.305500</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.210753</td>\n      <td>17</td>\n      <td>-0.616454</td>\n      <td>0.946362</td>\n      <td>-119.25300</td>\n      <td>4.08235</td>\n      <td>185.25700</td>\n      <td>1.383310</td>\n      <td>-47.521400</td>\n      <td>1.09130</td>\n      <td>...</td>\n      <td>9.66778</td>\n      <td>0.626942</td>\n      <td>1.49425</td>\n      <td>0.517513</td>\n      <td>-10.222100</td>\n      <td>2.627310</td>\n      <td>0.617270</td>\n      <td>1.456450</td>\n      <td>10.028800</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.439671</td>\n      <td>20</td>\n      <td>0.968126</td>\n      <td>-0.092546</td>\n      <td>74.30200</td>\n      <td>12.30650</td>\n      <td>72.18600</td>\n      <td>-0.233964</td>\n      <td>24.399100</td>\n      <td>1.10151</td>\n      <td>...</td>\n      <td>290.65700</td>\n      <td>15.604300</td>\n      <td>1.73557</td>\n      <td>-0.476668</td>\n      <td>1.390190</td>\n      <td>2.195740</td>\n      <td>0.826987</td>\n      <td>1.784850</td>\n      <td>7.071970</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>249995</th>\n      <td>0.923980</td>\n      <td>6</td>\n      <td>0.663212</td>\n      <td>-0.055120</td>\n      <td>6.61768</td>\n      <td>1.26619</td>\n      <td>40.44790</td>\n      <td>0.852375</td>\n      <td>25.522400</td>\n      <td>1.12294</td>\n      <td>...</td>\n      <td>3517.97000</td>\n      <td>18.534200</td>\n      <td>1.41936</td>\n      <td>0.607504</td>\n      <td>4.950780</td>\n      <td>2.853420</td>\n      <td>0.961882</td>\n      <td>1.068010</td>\n      <td>1.437650</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>249996</th>\n      <td>0.243556</td>\n      <td>7</td>\n      <td>-0.557062</td>\n      <td>1.333470</td>\n      <td>-54.88610</td>\n      <td>17.58310</td>\n      <td>212.96400</td>\n      <td>-1.204750</td>\n      <td>-128.385000</td>\n      <td>1.19334</td>\n      <td>...</td>\n      <td>1945.81000</td>\n      <td>19.482200</td>\n      <td>1.60072</td>\n      <td>-0.154877</td>\n      <td>5.693280</td>\n      <td>1.994540</td>\n      <td>0.760925</td>\n      <td>1.427880</td>\n      <td>1.226720</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>249997</th>\n      <td>0.046023</td>\n      <td>53</td>\n      <td>0.462863</td>\n      <td>0.704034</td>\n      <td>2062.94000</td>\n      <td>14.58160</td>\n      <td>11.42710</td>\n      <td>-0.509812</td>\n      <td>80.818000</td>\n      <td>1.20383</td>\n      <td>...</td>\n      <td>-28.46890</td>\n      <td>11.008200</td>\n      <td>1.61080</td>\n      <td>-0.350927</td>\n      <td>27.986900</td>\n      <td>1.012690</td>\n      <td>0.830533</td>\n      <td>1.005470</td>\n      <td>0.455332</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>249998</th>\n      <td>0.977330</td>\n      <td>12</td>\n      <td>-1.002880</td>\n      <td>0.576377</td>\n      <td>4741.16000</td>\n      <td>11.10090</td>\n      <td>3.81546</td>\n      <td>0.616191</td>\n      <td>118.902000</td>\n      <td>1.50424</td>\n      <td>...</td>\n      <td>-275.53700</td>\n      <td>21.985700</td>\n      <td>3.16051</td>\n      <td>0.772058</td>\n      <td>2.592220</td>\n      <td>3.938260</td>\n      <td>0.697430</td>\n      <td>0.962347</td>\n      <td>7.415890</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>249999</th>\n      <td>0.244233</td>\n      <td>51</td>\n      <td>0.729304</td>\n      <td>-0.702592</td>\n      <td>197.46200</td>\n      <td>19.02400</td>\n      <td>42.33370</td>\n      <td>0.465181</td>\n      <td>-115.398000</td>\n      <td>1.13675</td>\n      <td>...</td>\n      <td>5387.25000</td>\n      <td>38.574600</td>\n      <td>3.61783</td>\n      <td>-0.544969</td>\n      <td>-0.987695</td>\n      <td>1.871910</td>\n      <td>0.452516</td>\n      <td>0.935411</td>\n      <td>28.912200</td>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\n<p>250000 rows × 101 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df = df.drop(columns='id')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                  f0             f1             f2             f3  \\\n",
       "count  250000.000000  250000.000000  250000.000000  250000.000000   \n",
       "mean        0.511213      51.378476       0.107155       0.050010   \n",
       "std         0.307884      42.396636       1.322200       0.792368   \n",
       "min        -0.069273     -17.000000      -7.895580      -1.475560   \n",
       "25%         0.251287      18.000000      -0.611172      -0.719418   \n",
       "50%         0.514962      41.000000       0.253815       0.004099   \n",
       "75%         0.777323      75.000000       0.759249       0.765456   \n",
       "max         1.072070     273.000000       9.768590       1.680190   \n",
       "\n",
       "                  f4             f5             f6             f7  \\\n",
       "count  250000.000000  250000.000000  250000.000000  250000.000000   \n",
       "mean     3595.133426       8.205953     164.508753       0.375533   \n",
       "std      6072.401061       5.475723     183.335563       0.813597   \n",
       "min     -7589.280000      -3.291050     -40.967200      -4.143080   \n",
       "25%       163.864750       4.110127      27.894900      -0.026245   \n",
       "50%       943.000500       7.472445      91.005250       0.619862   \n",
       "75%      4115.355000      11.030950     240.843750       0.933855   \n",
       "max     37847.500000      35.078000     947.143000       4.010380   \n",
       "\n",
       "                  f8             f9  ...            f91            f92  \\\n",
       "count  250000.000000  250000.000000  ...  250000.000000  250000.000000   \n",
       "mean       16.669745       1.190382  ...    4856.812768      22.579100   \n",
       "std        99.758709       0.099700  ...    8501.609009      14.849390   \n",
       "min      -502.813000       0.934037  ...  -12695.700000      -4.059170   \n",
       "25%       -17.392025       1.132640  ...      73.203100      11.525450   \n",
       "50%         8.714945       1.170370  ...    1060.025000      19.993200   \n",
       "75%        55.407625       1.218880  ...    5572.982500      32.271625   \n",
       "max       465.956000       1.712450  ...   54334.600000      79.912400   \n",
       "\n",
       "                 f93            f94            f95            f96  \\\n",
       "count  250000.000000  250000.000000  250000.000000  250000.000000   \n",
       "mean        2.030554       0.079692       1.555097       2.417556   \n",
       "std         0.900211       0.587780       9.253785       0.892563   \n",
       "min         0.057800      -1.998800     -24.686300      -1.131980   \n",
       "25%         1.471650      -0.408975      -4.004925       1.906718   \n",
       "50%         1.660830       0.215710       0.759942       2.340430   \n",
       "75%         2.320085       0.503134       6.202502       2.910020   \n",
       "max         5.403020       1.944190      42.890400       5.576040   \n",
       "\n",
       "                 f97            f98            f99           loss  \n",
       "count  250000.000000  250000.000000  250000.000000  250000.000000  \n",
       "mean        0.537484       1.576900       8.048805       6.813920  \n",
       "std         0.226589       0.646306       5.647368       7.940179  \n",
       "min         0.005249      -0.646967      -0.842397       0.000000  \n",
       "25%         0.359646       1.215810       3.732800       1.000000  \n",
       "50%         0.531348       1.451285       7.182205       4.000000  \n",
       "75%         0.709807       1.901632      10.998550      10.000000  \n",
       "max         1.105400       4.492620      34.019200      42.000000  \n",
       "\n",
       "[8 rows x 101 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>f0</th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>f5</th>\n      <th>f6</th>\n      <th>f7</th>\n      <th>f8</th>\n      <th>f9</th>\n      <th>...</th>\n      <th>f91</th>\n      <th>f92</th>\n      <th>f93</th>\n      <th>f94</th>\n      <th>f95</th>\n      <th>f96</th>\n      <th>f97</th>\n      <th>f98</th>\n      <th>f99</th>\n      <th>loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>250000.000000</td>\n      <td>250000.000000</td>\n      <td>250000.000000</td>\n      <td>250000.000000</td>\n      <td>250000.000000</td>\n      <td>250000.000000</td>\n      <td>250000.000000</td>\n      <td>250000.000000</td>\n      <td>250000.000000</td>\n      <td>250000.000000</td>\n      <td>...</td>\n      <td>250000.000000</td>\n      <td>250000.000000</td>\n      <td>250000.000000</td>\n      <td>250000.000000</td>\n      <td>250000.000000</td>\n      <td>250000.000000</td>\n      <td>250000.000000</td>\n      <td>250000.000000</td>\n      <td>250000.000000</td>\n      <td>250000.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.511213</td>\n      <td>51.378476</td>\n      <td>0.107155</td>\n      <td>0.050010</td>\n      <td>3595.133426</td>\n      <td>8.205953</td>\n      <td>164.508753</td>\n      <td>0.375533</td>\n      <td>16.669745</td>\n      <td>1.190382</td>\n      <td>...</td>\n      <td>4856.812768</td>\n      <td>22.579100</td>\n      <td>2.030554</td>\n      <td>0.079692</td>\n      <td>1.555097</td>\n      <td>2.417556</td>\n      <td>0.537484</td>\n      <td>1.576900</td>\n      <td>8.048805</td>\n      <td>6.813920</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.307884</td>\n      <td>42.396636</td>\n      <td>1.322200</td>\n      <td>0.792368</td>\n      <td>6072.401061</td>\n      <td>5.475723</td>\n      <td>183.335563</td>\n      <td>0.813597</td>\n      <td>99.758709</td>\n      <td>0.099700</td>\n      <td>...</td>\n      <td>8501.609009</td>\n      <td>14.849390</td>\n      <td>0.900211</td>\n      <td>0.587780</td>\n      <td>9.253785</td>\n      <td>0.892563</td>\n      <td>0.226589</td>\n      <td>0.646306</td>\n      <td>5.647368</td>\n      <td>7.940179</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-0.069273</td>\n      <td>-17.000000</td>\n      <td>-7.895580</td>\n      <td>-1.475560</td>\n      <td>-7589.280000</td>\n      <td>-3.291050</td>\n      <td>-40.967200</td>\n      <td>-4.143080</td>\n      <td>-502.813000</td>\n      <td>0.934037</td>\n      <td>...</td>\n      <td>-12695.700000</td>\n      <td>-4.059170</td>\n      <td>0.057800</td>\n      <td>-1.998800</td>\n      <td>-24.686300</td>\n      <td>-1.131980</td>\n      <td>0.005249</td>\n      <td>-0.646967</td>\n      <td>-0.842397</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.251287</td>\n      <td>18.000000</td>\n      <td>-0.611172</td>\n      <td>-0.719418</td>\n      <td>163.864750</td>\n      <td>4.110127</td>\n      <td>27.894900</td>\n      <td>-0.026245</td>\n      <td>-17.392025</td>\n      <td>1.132640</td>\n      <td>...</td>\n      <td>73.203100</td>\n      <td>11.525450</td>\n      <td>1.471650</td>\n      <td>-0.408975</td>\n      <td>-4.004925</td>\n      <td>1.906718</td>\n      <td>0.359646</td>\n      <td>1.215810</td>\n      <td>3.732800</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.514962</td>\n      <td>41.000000</td>\n      <td>0.253815</td>\n      <td>0.004099</td>\n      <td>943.000500</td>\n      <td>7.472445</td>\n      <td>91.005250</td>\n      <td>0.619862</td>\n      <td>8.714945</td>\n      <td>1.170370</td>\n      <td>...</td>\n      <td>1060.025000</td>\n      <td>19.993200</td>\n      <td>1.660830</td>\n      <td>0.215710</td>\n      <td>0.759942</td>\n      <td>2.340430</td>\n      <td>0.531348</td>\n      <td>1.451285</td>\n      <td>7.182205</td>\n      <td>4.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.777323</td>\n      <td>75.000000</td>\n      <td>0.759249</td>\n      <td>0.765456</td>\n      <td>4115.355000</td>\n      <td>11.030950</td>\n      <td>240.843750</td>\n      <td>0.933855</td>\n      <td>55.407625</td>\n      <td>1.218880</td>\n      <td>...</td>\n      <td>5572.982500</td>\n      <td>32.271625</td>\n      <td>2.320085</td>\n      <td>0.503134</td>\n      <td>6.202502</td>\n      <td>2.910020</td>\n      <td>0.709807</td>\n      <td>1.901632</td>\n      <td>10.998550</td>\n      <td>10.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.072070</td>\n      <td>273.000000</td>\n      <td>9.768590</td>\n      <td>1.680190</td>\n      <td>37847.500000</td>\n      <td>35.078000</td>\n      <td>947.143000</td>\n      <td>4.010380</td>\n      <td>465.956000</td>\n      <td>1.712450</td>\n      <td>...</td>\n      <td>54334.600000</td>\n      <td>79.912400</td>\n      <td>5.403020</td>\n      <td>1.944190</td>\n      <td>42.890400</td>\n      <td>5.576040</td>\n      <td>1.105400</td>\n      <td>4.492620</td>\n      <td>34.019200</td>\n      <td>42.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 101 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "source": [
    "## Data Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = StandardScaler().fit_transform(df.drop(columns='loss'))\n",
    "y = df['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((250000, 100), (250000,))"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "source": [
    "## Neural Network Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(100,)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam',loss='mse', metrics=[RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "6250/6250 [==============================] - 5s 748us/step - loss: 63.3836 - root_mean_squared_error: 7.9614 - val_loss: 63.1434 - val_root_mean_squared_error: 7.9463\n",
      "Epoch 2/5\n",
      "6250/6250 [==============================] - 5s 756us/step - loss: 62.5097 - root_mean_squared_error: 7.9063 - val_loss: 62.9556 - val_root_mean_squared_error: 7.9345\n",
      "Epoch 3/5\n",
      "6250/6250 [==============================] - 5s 742us/step - loss: 62.2170 - root_mean_squared_error: 7.8878 - val_loss: 63.0053 - val_root_mean_squared_error: 7.9376\n",
      "Epoch 4/5\n",
      "6250/6250 [==============================] - 5s 739us/step - loss: 62.0153 - root_mean_squared_error: 7.8750 - val_loss: 63.2633 - val_root_mean_squared_error: 7.9538\n",
      "Epoch 5/5\n",
      "6250/6250 [==============================] - 5s 723us/step - loss: 61.8985 - root_mean_squared_error: 7.8676 - val_loss: 62.9649 - val_root_mean_squared_error: 7.9350\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d397a6e9a0>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "model.fit(X, y, epochs=5, validation_split=0.2)"
   ]
  },
  {
   "source": [
    "## Submit with test data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "X_test = StandardScaler().fit_transform(df_test.drop(columns='id'))\n",
    "\n",
    "y_pred = model.predict(X_test).ravel()\n",
    "\n",
    "pd.DataFrame({'id': df_test['id'], 'loss': y_pred}).to_csv('submit.csv', index=False)"
   ]
  }
 ]
}